{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of kaggle1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lfpczJKESQS"
      },
      "source": [
        "**IMAGE CLASSIFICATION USING CNN**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAODcU7QEio_"
      },
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rDz03J8cR1H"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import skimage.io as io\n",
        "from torchvision import datasets\n",
        "from skimage.transform import rotate, AffineTransform, warp\n",
        "from skimage.util import random_noise\n",
        "from skimage.filters import gaussian\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMqHow3XEnU4"
      },
      "source": [
        "Using the data uploaded on drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a3uPtG-3yHQ",
        "outputId": "e267b27d-a283-40a2-fa2d-af1310d0709d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuBSh7VtEr-U"
      },
      "source": [
        "Defining the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VjL3UmWvvNw"
      },
      "source": [
        "# Neural Network model\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # first convolution layer with batch normalization\n",
        "        self.conv1 = nn.Conv2d(3,6,5)\n",
        "        self.conv1_bn = nn.BatchNorm2d(6)\n",
        "        # second convolution layer with batch normalization\n",
        "        self.conv2 = nn.Conv2d(6,12,5)\n",
        "        self.conv2_bn = nn.BatchNorm2d(12)\n",
        "        # third convolution layer with batch normalization\n",
        "        self.conv3 = nn.Conv2d(12,32,3)\n",
        "        self.conv3_bn = nn.BatchNorm2d(32)\n",
        "        # fourth convolution layer with batch normalization\n",
        "        self.conv4 = nn.Conv2d(32,128,3)\n",
        "        self.conv4_bn = nn.BatchNorm2d(128)\n",
        "        # fifth convolution layer with batch normalization\n",
        "        self.conv5 = nn.Conv2d(128,256,3)\n",
        "        self.conv5_bn = nn.BatchNorm2d(256)\n",
        "        # first fully connected layer with batch normalization\n",
        "        self.fcn1 = nn.Linear(256*5*5,4000)\n",
        "        self.fcn1_bn = nn.BatchNorm1d(4000)\n",
        "        # second fully connected layer with batch normalization\n",
        "        self.fcn2 = nn.Linear(4000,1000)\n",
        "        self.fcn2_bn = nn.BatchNorm1d(1000)\n",
        "        # third fully connected layer with batch normalization\n",
        "        self.fcn3 = nn.Linear(1000,500)\n",
        "        self.fcn3_bn = nn.BatchNorm1d(500)\n",
        "        # final output\n",
        "        self.out = nn.Linear(500,10)\n",
        "        # dropout\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self,x):\n",
        "        # first convolution layer with max pooling and relu activation\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv1_bn(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        # second convolution layer with max pooling and relu activation\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv2_bn(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        # third convolution layer with max pooling and relu activation\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv3_bn(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        # fourth convolution layer with max pooling and relu activation\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv4_bn(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        # fifth convolution layer with max pooling and relu activation\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv5_bn(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        # reshaping for fully connected layer\n",
        "        x = x.reshape(-1, 256*5*5)\n",
        "        # first fully connected layer with max pooling, dropout and relu activation\n",
        "        x = self.fcn1(x)\n",
        "        x = self.fcn1_bn(x)\n",
        "        x = F.relu(x)\n",
        "        # second fully connected layer with max pooling, dropout and relu activation\n",
        "        x = self.dropout(x)\n",
        "        x = self.fcn2(x)\n",
        "        x = self.fcn2_bn(x)\n",
        "        x = F.relu(x)\n",
        "        # third fully connected layer with max pooling, dropout and relu activation\n",
        "        x = self.dropout(x)\n",
        "        x = self.fcn3(x)\n",
        "        x = self.fcn3_bn(x)\n",
        "        x = F.relu(x)\n",
        "        # output\n",
        "        x = self.out(x)\n",
        "        # applying softmax\n",
        "        x = F.softmax(x,dim=1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-2VguNT4mTP"
      },
      "source": [
        "# creating a Network\n",
        "network = Network().cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky-hNMIyEysT"
      },
      "source": [
        "Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZLN4Qy1_dgy"
      },
      "source": [
        "# Augmenting images and saving them in same folder\n",
        "!pip install Augmentor\n",
        "import Augmentor\n",
        "# Augmentation for railway folder\n",
        "p = Augmentor.Pipeline('/content/drive/My Drive/GNR638/TrainSet/train/railway',output_directory='/content/drive/My Drive/GNR638/TrainSet/train/railway')\n",
        "# flip left/right\n",
        "p.flip_left_right(probability=0.8)\n",
        "# flip top/bottom\n",
        "p.flip_top_bottom(probability=0.8)\n",
        "# rotate \n",
        "p.rotate_random_90(probability=0.8)\n",
        "# shear\n",
        "p.shear(probability=0.8,max_shear_left=20,max_shear_right=20)\n",
        "# zoom\n",
        "p.zoom(probability=0.8,min_factor=1.1, max_factor=1.8)\n",
        "# skew\n",
        "p.skew(probability=0.8)\n",
        "p.sample(500)\n",
        "p = Augmentor.Pipeline('/content/drive/My Drive/GNR638/TrainSet/train/swimming_pool',output_directory='/content/drive/My Drive/GNR638/TrainSet/train/swimming_pool')\n",
        "# flip left/right\n",
        "p.flip_left_right(probability=0.8)\n",
        "# flip top/bottom\n",
        "p.flip_top_bottom(probability=0.8)\n",
        "# rotate \n",
        "p.rotate_random_90(probability=0.8)\n",
        "# shear\n",
        "p.shear(probability=0.8,max_shear_left=20,max_shear_right=20)\n",
        "# zoom\n",
        "p.zoom(probability=0.8,min_factor=1.1, max_factor=1.8)\n",
        "# skew\n",
        "p.skew(probability=0.8)\n",
        "p.sample(500)\n",
        "\n",
        "p = Augmentor.Pipeline('/content/drive/My Drive/GNR638/TrainSet/train/tennis_court',output_directory='/content/drive/My Drive/GNR638/TrainSet/train/tennis_court')\n",
        "# flip left/right\n",
        "p.flip_left_right(probability=0.8)\n",
        "# flip top/bottom\n",
        "p.flip_top_bottom(probability=0.8)\n",
        "# rotate\n",
        "p.rotate_random_90(probability=0.8)\n",
        "# shear\n",
        "p.shear(probability=0.8,max_shear_left=20,max_shear_right=20)\n",
        "# zoom\n",
        "p.zoom(probability=0.8,min_factor=1.1, max_factor=1.8)\n",
        "# skew\n",
        "p.skew(probability=0.8)\n",
        "p.sample(500)\n",
        "\n",
        "p = Augmentor.Pipeline('/content/drive/My Drive/GNR638/TrainSet/train/bridge',output_directory='/content/drive/My Drive/GNR638/TrainSet/train/bridge')\n",
        "# flip left/right\n",
        "p.flip_left_right(probability=0.8)\n",
        "# flip top/bottom\n",
        "p.flip_top_bottom(probability=0.8)\n",
        "# rotate\n",
        "p.rotate_random_90(probability=0.8)\n",
        "# shear\n",
        "p.shear(probability=0.8,max_shear_left=20,max_shear_right=20)\n",
        "# zoom\n",
        "p.zoom(probability=0.8,min_factor=1.1, max_factor=1.8)\n",
        "# skew\n",
        "p.skew(probability=0.8)\n",
        "p.sample(500)\n",
        "\n",
        "p = Augmentor.Pipeline('/content/drive/My Drive/GNR638/TrainSet/train/oil_well',output_directory='/content/drive/My Drive/GNR638/TrainSet/train/oil_well')\n",
        "# flip left/right\n",
        "p.flip_left_right(probability=0.8)\n",
        "# flip top/bottom\n",
        "p.flip_top_bottom(probability=0.8)\n",
        "# rotate\n",
        "p.rotate_random_90(probability=0.8)\n",
        "# shear\n",
        "p.shear(probability=0.8,max_shear_left=20,max_shear_right=20)\n",
        "# zoom\n",
        "p.zoom(probability=0.8,min_factor=1.1, max_factor=1.8)\n",
        "# skew\n",
        "p.skew(probability=0.8)\n",
        "p.sample(500)\n",
        "\n",
        "p = Augmentor.Pipeline('/content/drive/My Drive/GNR638/TrainSet/train/crosswalk',output_directory='/content/drive/My Drive/GNR638/TrainSet/train/crosswalk')\n",
        "# flip left/right\n",
        "p.flip_left_right(probability=0.8)\n",
        "# flip top/bottom\n",
        "p.flip_top_bottom(probability=0.8)\n",
        "# rotate\n",
        "p.rotate_random_90(probability=0.8)\n",
        "# shear\n",
        "p.shear(probability=0.8,max_shear_left=20,max_shear_right=20)\n",
        "# zoom\n",
        "p.zoom(probability=0.8,min_factor=1.1, max_factor=1.8)\n",
        "# skew\n",
        "p.skew(probability=0.8)\n",
        "p.sample(500)\n",
        "\n",
        "p = Augmentor.Pipeline('/content/drive/My Drive/GNR638/TrainSet/train/overpass',output_directory='/content/drive/My Drive/GNR638/TrainSet/train/overpass')\n",
        "# flip left/right\n",
        "p.flip_left_right(probability=0.8)\n",
        "# flip top/bottom\n",
        "p.flip_top_bottom(probability=0.8)\n",
        "# rotate\n",
        "p.rotate_random_90(probability=0.8)\n",
        "# shear\n",
        "p.shear(probability=0.8,max_shear_left=20,max_shear_right=20)\n",
        "# zoom\n",
        "p.zoom(probability=0.8,min_factor=1.1, max_factor=1.8)\n",
        "# skew\n",
        "p.skew(probability=0.8)\n",
        "p.sample(500)\n",
        "\n",
        "p = Augmentor.Pipeline('/content/drive/My Drive/GNR638/TrainSet/train/basketball_court',output_directory='/content/drive/My Drive/GNR638/TrainSet/train/basketball_court')\n",
        "# flip left/right\n",
        "p.flip_left_right(probability=0.8)\n",
        "# flip top/bottom\n",
        "p.flip_top_bottom(probability=0.8)\n",
        "# rotate\n",
        "p.rotate_random_90(probability=0.8)\n",
        "# shear\n",
        "p.shear(probability=0.8,max_shear_left=20,max_shear_right=20)\n",
        "# zoom\n",
        "p.zoom(probability=0.8,min_factor=1.1, max_factor=1.8)\n",
        "# skew\n",
        "p.skew(probability=0.8)\n",
        "p.sample(500)\n",
        "\n",
        "p = Augmentor.Pipeline('/content/drive/My Drive/GNR638/TrainSet/train/runway',output_directory='/content/drive/My Drive/GNR638/TrainSet/train/runway')\n",
        "# flip left/right\n",
        "p.flip_left_right(probability=0.8)\n",
        "# flip top/bottom\n",
        "p.flip_top_bottom(probability=0.8)\n",
        "# rotate\n",
        "p.rotate_random_90(probability=0.8)\n",
        "# shear\n",
        "p.shear(probability=0.8,max_shear_left=20,max_shear_right=20)\n",
        "# zoom\n",
        "p.zoom(probability=0.8,min_factor=1.1, max_factor=1.8)\n",
        "# skew\n",
        "p.skew(probability=0.8)\n",
        "p.sample(500)\n",
        "\n",
        "p = Augmentor.Pipeline('/content/drive/My Drive/GNR638/TrainSet/train/golf_course',output_directory='/content/drive/My Drive/GNR638/TrainSet/train/golf_course')\n",
        "# flip left/right\n",
        "p.flip_left_right(probability=0.8)\n",
        "# flip top/bottom\n",
        "p.flip_top_bottom(probability=0.8)\n",
        "# rotate\n",
        "p.rotate_random_90(probability=0.8)\n",
        "# shear\n",
        "p.shear(probability=0.8,max_shear_left=20,max_shear_right=20)\n",
        "# zoom\n",
        "p.zoom(probability=0.8,min_factor=1.1, max_factor=1.8)\n",
        "# skew\n",
        "p.skew(probability=0.8)\n",
        "p.sample(500)\n",
        "\n",
        "# Now we will have 500 + 10*500 = 5500 images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhsCDhtxE2qj"
      },
      "source": [
        "Training the given data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARDHSBmiH7DZ"
      },
      "source": [
        "# training dataset\n",
        "train_set = datasets.ImageFolder(root='/content/drive/My Drive/GNR638/TrainSet/train', transform=transforms.Compose([transforms.ToTensor()]), target_transform=None, is_valid_file=None)\n",
        "# using batch size = 10\n",
        "Training_dataset = torch.utils.data.DataLoader(train_set, batch_size = 10, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZSLSi-c5Oni",
        "outputId": "17fc5f13-29e1-41c8-80fb-6174b1e7f287"
      },
      "source": [
        "# using stochastic gradient descent as optimizer and cross entroply loss as loss function\n",
        "optimizer = optim.SGD(network.parameters(), lr=0.05,momentum=0.6)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "# test set\n",
        "test_set = datasets.ImageFolder('/content/drive/My Drive/GNR638/TestSet', transform=transforms.Compose([transforms.ToTensor()]), target_transform=None, is_valid_file=None)\n",
        "final_test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False)\n",
        "maxm = 0\n",
        "# test labels\n",
        "label_test_set = torch.tensor(np.array([4,9,2,4,6,0,9,3,1,1,6,0,4,5,7,8,0,4,7,9,4,1,5,3,2,5,6,6,4,2,8,0,2,7,1,3,2,1,5,8,9,9,0,3,6,8,9,4,0,8,7,7,0,5,0,3,9,2,6,1,9,5,8,1,6,3,7,8,3,9,4,5,8,1,7,2,6,0,2,8,1,3,1,7,5,9,7,2,4,2,5,3,5,8,6,6,4,3,0,7]))\n",
        "# epochs = 50\n",
        "for epoch in range(50):\n",
        "    # initializing train accuracy and total running loss\n",
        "    total_loss = 0\n",
        "    train_acc = 0\n",
        "    for batch in Training_dataset:\n",
        "        images, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        preds = network(images.cuda())\n",
        "        loss = loss_func(preds.cuda(), labels.cuda())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss = total_loss + loss.item()\n",
        "        train_acc += preds.cuda().argmax(dim=1).eq(labels.cuda()).sum().item()\n",
        "    result = torch.tensor([])\n",
        "    for batch1 in final_test_loader:\n",
        "        image1,label1 = batch1\n",
        "        prediction = network(image1.cuda())\n",
        "        result = prediction.argmax(dim=1)\n",
        "    if torch.eq(label_test_set.cuda(), result).sum().item()>=maxm:\n",
        "        maxm = torch.eq(label_test_set.cuda(), result).sum().item()\n",
        "        # saving the model\n",
        "        torch.save(network, 'model.pkl')\n",
        "    print(epoch)\n",
        "    print(train_acc*100/5500)\n",
        "    print(total_loss)    \n",
        "    # test accuracy\n",
        "    print(torch.eq(label_test_set.cuda(), result).sum().item(),'%')\n",
        "    print('---------------------------------------------------')    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "48.763636363636365\n",
            "1085.2134841680527\n",
            "42 %\n",
            "---------------------------------------------------\n",
            "1\n",
            "62.30909090909091\n",
            "1012.2746584415436\n",
            "48 %\n",
            "---------------------------------------------------\n",
            "2\n",
            "66.69090909090909\n",
            "986.998117685318\n",
            "66 %\n",
            "---------------------------------------------------\n",
            "3\n",
            "70.74545454545455\n",
            "964.4654704332352\n",
            "65 %\n",
            "---------------------------------------------------\n",
            "4\n",
            "71.03636363636363\n",
            "962.0825779438019\n",
            "68 %\n",
            "---------------------------------------------------\n",
            "5\n",
            "74.69090909090909\n",
            "943.6329208612442\n",
            "72 %\n",
            "---------------------------------------------------\n",
            "6\n",
            "78.49090909090908\n",
            "924.651504278183\n",
            "75 %\n",
            "---------------------------------------------------\n",
            "7\n",
            "78.4\n",
            "922.4520336389542\n",
            "69 %\n",
            "---------------------------------------------------\n",
            "8\n",
            "80.78181818181818\n",
            "909.8121120929718\n",
            "80 %\n",
            "---------------------------------------------------\n",
            "9\n",
            "80.65454545454546\n",
            "909.7443891763687\n",
            "72 %\n",
            "---------------------------------------------------\n",
            "10\n",
            "82.87272727272727\n",
            "898.2116483449936\n",
            "73 %\n",
            "---------------------------------------------------\n",
            "11\n",
            "83.21818181818182\n",
            "897.0724345445633\n",
            "74 %\n",
            "---------------------------------------------------\n",
            "12\n",
            "84.27272727272727\n",
            "890.8389365673065\n",
            "77 %\n",
            "---------------------------------------------------\n",
            "13\n",
            "84.10909090909091\n",
            "890.2800179719925\n",
            "74 %\n",
            "---------------------------------------------------\n",
            "14\n",
            "84.38181818181818\n",
            "889.6342287063599\n",
            "76 %\n",
            "---------------------------------------------------\n",
            "15\n",
            "86.27272727272727\n",
            "880.9312757253647\n",
            "79 %\n",
            "---------------------------------------------------\n",
            "16\n",
            "84.72727272727273\n",
            "887.9412221908569\n",
            "78 %\n",
            "---------------------------------------------------\n",
            "17\n",
            "86.78181818181818\n",
            "876.7686887979507\n",
            "79 %\n",
            "---------------------------------------------------\n",
            "18\n",
            "87.12727272727273\n",
            "875.0138043165207\n",
            "76 %\n",
            "---------------------------------------------------\n",
            "19\n",
            "87.4\n",
            "874.1106307506561\n",
            "78 %\n",
            "---------------------------------------------------\n",
            "20\n",
            "88.0\n",
            "869.9907463788986\n",
            "74 %\n",
            "---------------------------------------------------\n",
            "21\n",
            "87.50909090909092\n",
            "872.1885237693787\n",
            "74 %\n",
            "---------------------------------------------------\n",
            "22\n",
            "89.23636363636363\n",
            "863.1497398614883\n",
            "80 %\n",
            "---------------------------------------------------\n",
            "23\n",
            "88.94545454545455\n",
            "864.3713126182556\n",
            "70 %\n",
            "---------------------------------------------------\n",
            "24\n",
            "89.2909090909091\n",
            "863.4985181093216\n",
            "76 %\n",
            "---------------------------------------------------\n",
            "25\n",
            "90.03636363636363\n",
            "859.0567418336868\n",
            "80 %\n",
            "---------------------------------------------------\n",
            "26\n",
            "90.47272727272727\n",
            "856.567365527153\n",
            "70 %\n",
            "---------------------------------------------------\n",
            "27\n",
            "90.47272727272727\n",
            "856.6279907226562\n",
            "76 %\n",
            "---------------------------------------------------\n",
            "28\n",
            "90.30909090909091\n",
            "857.7845352888107\n",
            "67 %\n",
            "---------------------------------------------------\n",
            "29\n",
            "90.65454545454546\n",
            "854.9524207115173\n",
            "79 %\n",
            "---------------------------------------------------\n",
            "30\n",
            "91.76363636363637\n",
            "849.0673574209213\n",
            "83 %\n",
            "---------------------------------------------------\n",
            "31\n",
            "92.47272727272727\n",
            "846.0620859861374\n",
            "83 %\n",
            "---------------------------------------------------\n",
            "32\n",
            "92.36363636363636\n",
            "846.7274236679077\n",
            "75 %\n",
            "---------------------------------------------------\n",
            "33\n",
            "92.61818181818182\n",
            "845.8214423656464\n",
            "78 %\n",
            "---------------------------------------------------\n",
            "34\n",
            "93.25454545454545\n",
            "841.744904756546\n",
            "76 %\n",
            "---------------------------------------------------\n",
            "35\n",
            "93.18181818181819\n",
            "842.1456967592239\n",
            "83 %\n",
            "---------------------------------------------------\n",
            "36\n",
            "92.67272727272727\n",
            "845.0504047870636\n",
            "78 %\n",
            "---------------------------------------------------\n",
            "37\n",
            "93.58181818181818\n",
            "840.150218963623\n",
            "89 %\n",
            "---------------------------------------------------\n",
            "38\n",
            "93.34545454545454\n",
            "840.8758709430695\n",
            "78 %\n",
            "---------------------------------------------------\n",
            "39\n",
            "93.63636363636364\n",
            "839.9375529289246\n",
            "82 %\n",
            "---------------------------------------------------\n",
            "40\n",
            "93.41818181818182\n",
            "839.9164845943451\n",
            "80 %\n",
            "---------------------------------------------------\n",
            "41\n",
            "93.96363636363637\n",
            "837.7075039148331\n",
            "80 %\n",
            "---------------------------------------------------\n",
            "42\n",
            "94.49090909090908\n",
            "834.1861974000931\n",
            "75 %\n",
            "---------------------------------------------------\n",
            "43\n",
            "94.78181818181818\n",
            "832.6574274301529\n",
            "78 %\n",
            "---------------------------------------------------\n",
            "44\n",
            "95.16363636363636\n",
            "830.9792251586914\n",
            "80 %\n",
            "---------------------------------------------------\n",
            "45\n",
            "95.25454545454545\n",
            "830.5996599197388\n",
            "78 %\n",
            "---------------------------------------------------\n",
            "46\n",
            "95.0\n",
            "832.6233701705933\n",
            "75 %\n",
            "---------------------------------------------------\n",
            "47\n",
            "95.25454545454545\n",
            "830.4715912342072\n",
            "75 %\n",
            "---------------------------------------------------\n",
            "48\n",
            "95.23636363636363\n",
            "830.8728364706039\n",
            "80 %\n",
            "---------------------------------------------------\n",
            "49\n",
            "94.54545454545455\n",
            "833.0847699642181\n",
            "79 %\n",
            "---------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7okPFRAdE9ED"
      },
      "source": [
        "Testing on the test set given"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjd9gPbmT3vU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a5575eb-b213-4bc2-89b9-1e14306c8350"
      },
      "source": [
        "# loading the saved model as network\n",
        "network = torch.load('model.pkl')\n",
        "test_set = datasets.ImageFolder('/content/drive/My Drive/GNR638/TestSet', transform=transforms.Compose([transforms.ToTensor()]), target_transform=None, is_valid_file=None)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False)\n",
        "labels = torch.tensor(np.array([4,9,2,4,6,0,9,3,1,1,6,0,4,5,7,8,0,4,7,9,4,1,5,3,2,5,6,6,4,2,8,0,2,7,1,3,2,1,5,8,9,9,0,3,6,8,9,4,0,8,7,7,0,5,0,3,9,2,6,1,9,5,8,1,6,3,7,8,3,9,4,5,8,1,7,2,6,0,2,8,1,3,1,7,5,9,7,2,4,2,5,3,5,8,6,6,4,3,0,7]))\n",
        "result = torch.tensor([])\n",
        "for batch1 in test_loader:\n",
        "    image1,label1 = batch1\n",
        "    prediction = network(image1.cuda())\n",
        "    result = prediction.argmax(dim=1)\n",
        "result = result + 1\n",
        "a = {'ImageID': range(101, 201), 'LabelID': result.tolist()}\n",
        "b = pd.DataFrame(a)\n",
        "# printing test accuracy\n",
        "print(result.tolist())\n",
        "# saving the csv file\n",
        "b.to_csv('/content/drive/My Drive/GNR638/19D070014.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90 %\n",
            "[5, 10, 3, 5, 7, 1, 10, 4, 2, 2, 1, 1, 5, 6, 8, 9, 6, 5, 8, 8, 5, 6, 6, 4, 3, 6, 7, 7, 5, 3, 9, 3, 3, 8, 6, 4, 3, 2, 6, 9, 3, 10, 1, 4, 7, 9, 10, 5, 1, 9, 8, 8, 1, 6, 1, 4, 10, 3, 7, 2, 10, 6, 9, 2, 7, 4, 8, 9, 4, 10, 5, 6, 9, 2, 8, 3, 10, 8, 3, 9, 2, 4, 2, 8, 6, 10, 8, 3, 5, 3, 6, 4, 6, 9, 7, 7, 5, 4, 8, 8]\n"
          ]
        }
      ]
    }
  ]
}